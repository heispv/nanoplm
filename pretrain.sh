myplm pretrain mlm \
    --train-file output/data/split/train.fasta \
    --val-file output/data/split/val.fasta \
    --output-dir output/mlm_checkpoints \
    --embed-dim 256 \
    --num-layers 6 \
    --num-heads 8 \
    --mlp-activation swiglu \
    --max-seq-len 512 \
    --min-seq-len 20 \
    --subsample-ratio 1.0 \
    --mlm-probability 0.15 \
    --mask-token-prob 0.8 \
    --random-token-prob 0.1 \
    --num-epochs 3 \
    --batch-size 16 \
    --learning-rate 5e-4 \
    --weight-decay 0.01 \
    --warmup-steps 500 \
    --save-steps 1000 \
    --eval-steps 500 \
    --logging-steps 100 \
    --save-total-limit 3 \
    --gradient-accumulation-steps 1 \
    --dataloader-num-workers 0 \
    --seed 42 \
    --device auto \
    --wandb-project protx_pretraining
